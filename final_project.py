# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzr7G5370jIaYGz_ZGIdMHjbIusW1bVK

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

df = pd.read_csv('/content/loan_data.csv')
# pd.set_option('display.max_columns', None)

"""# Data Understanding"""

df.info()
df.head()

"""## Missing Value Check"""

missing_value = df.isnull().sum()
missing_value

"""## Duplicate Check"""

duplicate = df.duplicated().sum()
duplicate

"""## Data Distribution"""

df.describe()

"""
# Exploratory Data Analysis (EDA)

## Univariate Analysis
"""

# hitogram plot for education, home ownership, and loan intention

plt.figure(figsize=(15, 12))

categorical_columns = ['person_education', 'person_home_ownership', 'loan_intent']

for i, column in enumerate(categorical_columns, 1):
    # Calculate the frequency and sort the categories.
    ordered_categories = df[column].value_counts().index

    # ordered categorical column.
    df[column] = pd.Categorical(df[column], categories=ordered_categories, ordered=True)

    # histogram plot
    plt.subplot(3, 3, i)
    ax = sns.histplot(df[column], bins=len(ordered_categories), kde=False, color='skyblue', edgecolor='black')

    # labeled the histogram
    for p in ax.patches:
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()
        ax.text(x, y, int(y), ha='center', va='bottom', fontsize=10)

    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frekuensi')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Piechart for gender dan loan default

# Create a figure plotting.
plt.figure(figsize=(10, 6))

# Pie chart for person_gender
plt.subplot(1, 2, 1)
gender_counts = df['person_gender'].value_counts()
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=['skyblue', 'lightcoral'], startangle=90)
plt.title('Distribution of Gender')

# Pie chart for previous_loan_defaults_on_file
plt.subplot(1, 2, 2)
loan_defaults_counts = df['previous_loan_defaults_on_file'].value_counts()
plt.pie(loan_defaults_counts, labels=loan_defaults_counts.index, autopct='%1.1f%%', colors=['lightgreen', 'orange'], startangle=90)
plt.title('Distribution of Previous Loan Defaults on File')

# show the plot
plt.tight_layout()
plt.show()

# boxplot for outlier

numerical_columns = ['person_age', 'person_income', 'person_emp_exp',
                     'loan_amnt','loan_int_rate','loan_percent_income',
                     'cb_person_cred_hist_length','credit_score','loan_status']

plt.figure(figsize=(15, 12))

for i, column in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=df, y=column, color='skyblue')
    plt.title(f'Boxplot of {column}')
    plt.xlabel('')
    plt.ylabel(column)

plt.tight_layout()
plt.show()

"""
# Data Cleaning

## Outlier Handling
"""

# person_age and person_emp_exp

age_exp = ((df['person_age'] >= 70) | (df['person_emp_exp'] >= 50))
print(f'The number of outliers in age and work experience: {age_exp.sum()}')
print(f'The percentage of outliers in age and work experience: {age_exp.mean()*100:.2f}%')

# handling by removing the outlier

df = df[~((df['person_age'] >= 70) | (df['person_emp_exp'] >= 50))]

# cb_person_cred_hist_length

data = df['cb_person_cred_hist_length'] > 20
print(f'The number of credit length over 20 years : {data.sum()}')
print(f'The percentage of credit length over 20 years : {data.mean()*100:.2f}%')

# handling by removing the outlier

df = df[~(df['cb_person_cred_hist_length'] > 20)]

"""
# Deep Dive Analysis

## KDE plot
"""

# descriptive labels for loan_status
df['loan_status'] = df['loan_status'].map({0: 'Rejected', 1: 'Approved'})

# List of numerical variable
numeric_columns = [
    'person_age', 'person_income', 'person_emp_exp', 'loan_amnt',
    'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score'
]

# create grid plot
plt.figure(figsize=(12, 14))
n_cols = 2
n_rows = (len(numeric_columns) + 1) // n_cols

# iteration for KDE plot
for i, column in enumerate(numeric_columns, 1):
    plt.subplot(n_rows, n_cols, i)

    # KDE plot
    sns.kdeplot(
        data=df,
        x=column,
        hue='loan_status',
        fill=True,
        alpha=0.5,
        palette='Set2'
    )

    # title and labels
    plt.title(f'Distribution of {column} by Loan Status', fontsize=12)
    plt.xlabel(column.replace('_', ' ').title(), fontsize=10)
    plt.ylabel('Density', fontsize=10)

# layout
plt.tight_layout()
plt.show()

"""
## Stake Barchart
"""

# loan_status colours
colors = ['lightseagreen', 'salmon']

# List of categorical variables
categorical_columns = [
    'person_gender', 'person_education', 'person_home_ownership',
    'loan_intent', 'previous_loan_defaults_on_file'
]

# Size and layout grid plot
plt.figure(figsize=(14, 13))
n_cols = 2
n_rows = (len(categorical_columns) + 1) // n_cols

# Plot each category
for i, column in enumerate(categorical_columns, 1):
    plt.subplot(n_rows, n_cols, i)

    # Create a pivot table
    pivot = pd.crosstab(df[column], df['loan_status'], normalize='index') * 100

    # Stacked bar chart
    bottom_values = [0] * len(pivot)
    for j, category in enumerate(pivot.columns):
        plt.bar(
            pivot.index,
            pivot[category],
            bottom=bottom_values,
            color=colors[j],
            label=category
        )
        bottom_values += pivot[category]

        # labels
        for k in range(len(pivot[category])):
            value = pivot[category].iloc[k]
            if value > 0:
                plt.text(
                    k,
                    bottom_values[k] - value / 2,
                    f"{value:.1f}%",
                    ha='center',
                    va='center',
                    fontsize=9
                )

    # Visual settings
    plt.title(f'{column.replace("_", " ").title()} by Loan Status', fontsize=12)
    plt.xlabel(column.replace('_', ' ').title(), fontsize=10)
    plt.ylabel('Percentage (%)', fontsize=10)
    plt.xticks(rotation=45)

# Add a legend
plt.legend(title='Loan Status', bbox_to_anchor=(1.05, 1), loc='upper left')

# Layout
plt.tight_layout()
plt.show()

"""
# Pre-Processing Data

## VIF score
"""

# retransform loan_status data type to binary

df['loan_status'] = df['loan_status'].map({'Rejected': 0 , 'Approved': 1})

# split the feature and target

X = df.drop('loan_status', axis=1)
Y = df['loan_status']

# Multicollinearity check

numeric = df.select_dtypes(include=['number'])

x = add_constant(numeric)

vif_df = pd.DataFrame([vif(x.values, i) for i in range(x.shape[1])],
                      index=x.columns).reset_index()
vif_df.columns = ['feature', 'vif_score']

vif_df = vif_df.loc[vif_df.feature != 'const']
vif_df

"""## Heatmap Corelation"""

# heatmap correlation

numeric = pd.concat([numeric], axis=1)
corr = numeric.corr()

plt.figure(figsize=(10,7))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.show()

# drop feature person_age and cb_person_cred_hist_length
columns_to_drop = ['person_age', 'cb_person_cred_hist_length']

# delete features from data frame
df = df.drop(columns=columns_to_drop, axis=1)

# Multicollinearity re-check
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

numeric = df.select_dtypes(include=['number'])

x = add_constant(numeric)

vif_df = pd.DataFrame([vif(x.values, i) for i in range(x.shape[1])],
                      index=x.columns).reset_index()
vif_df.columns = ['feature', 'vif_score']

vif_df = vif_df.loc[vif_df.feature != 'const']
vif_df

"""## Features Encoding"""

# transform data type 'person_gender'
df['person_gender'] = df['person_gender'].map({'male':1, 'female': 0})

# transform data type 'previous_loan_defaults_on_file'
df['previous_loan_defaults_on_file'] = df['previous_loan_defaults_on_file'].map({'Yes': 1, 'No': 0})

# unique data check person_education
unique1 = df['person_education'].unique()
unique1

# transform data type 'person_education'
df['person_education'] = df['person_education'].map({
    'High School': 0,
    'Associate': 1,
    'Bachelor': 2,
    'Master': 3,
    'Doctorate': 4
    })

# unique data check 'person_home_ownership'
unique2 = df['person_home_ownership'].unique()
unique2

# transform data type 'person_home_ownership'
df['person_home_ownership'] = df['person_home_ownership'].map({
    'OTHER': 0,
    'RENT': 0,
    'OWN': 1,
    'MORTGAGE': 1
    })

# unique data check 'loan_intent'
unique3 = df['loan_intent'].unique()
unique3

# transform data type 'loan_intent'
df['loan_intent'] = df['loan_intent'].map({
    'PERSONAL': 0,
    'MEDICAL': 0,
    'DEBTCONSOLIDATION': 0,
    'EDUCATION': 1,
    'VENTURE': 1,
    'HOMEIMPROVEMENT': 1
    })

"""## Feature Importance"""

X = df.drop(columns=['loan_status'])
Y = df['loan_status']

# Building and training a Random Forest model
model = RandomForestClassifier()
model.fit(X, Y)

# feature importance
feature_importances = model.feature_importances_

# frame
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""# Modelling"""

# split the data (data train and data test)

X = df.drop('loan_status', axis=1)
Y = df['loan_status']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

"""
## K-Nearest Neighbors (KNN)

### KNN 11 features
"""

# Initialize and train the model

k = 3
model_knn = KNeighborsClassifier(n_neighbors=k)
model_knn.fit(X_train, Y_train)

# Metrics Evaluation

Y_train_pred = model_knn.predict(X_train)
Y_test_pred = model_knn.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# plotting confusion matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### KNN 4 features"""

new_column = ['credit_score', 'loan_amnt','person_home_ownership','person_emp_exp','person_education','loan_intent','person_gender']

features_train = X_train.drop(columns = new_column, axis = 1)
target_train = Y_train

features_test = X_test.drop(columns = new_column, axis = 1)
target_test = Y_test

# Initialize and train the model

k = 3
model_knn = KNeighborsClassifier(n_neighbors=k)
model_knn.fit(features_train, target_train)

# Metrics Evaluation

target_train_pred = model_knn.predict(features_train)
target_test_pred = model_knn.predict(features_test)

train_accuracy = accuracy_score(target_train, Y_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# plotting confusion matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""
## Logistic Regression

### Logistic Regression 11 features
"""

# Initialize and train the model

model_lr = LogisticRegression(max_iter=1000, random_state=42)
model_lr.fit(X_train, Y_train)

# Metrics Evaluation

Y_train_pred = model_lr.predict(X_train)
Y_test_pred = model_lr.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### LogisticRegression 4 features"""

# Initialize and train the model

model_lr_new = LogisticRegression(max_iter=1000, random_state=42)
model_lr_new.fit(features_train, target_train)

# Metrics Evaluation

target_train_pred = model_lr_new.predict(features_train)
target_test_pred = model_lr_new.predict(features_test)

train_accuracy = accuracy_score(target_train, target_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""
## Decission Tree

### Decision Tree 11 features
"""

# Initialize and train the model

model_dt = DecisionTreeClassifier(criterion='gini', max_depth=9, random_state=42)
model_dt.fit(X_train, Y_train)

# Metrics Evaluation

Y_train_pred = model_dt.predict(X_train)
Y_test_pred = model_dt.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### Decission Tree 4 features"""

# Initialize and train the model

model_dt = DecisionTreeClassifier(criterion='gini', max_depth=9, random_state=42)
model_dt.fit(features_train, target_train)

# Metrics Evaluation

target_train_pred = model_dt.predict(features_train)
target_test_pred = model_dt.predict(features_test)

train_accuracy = accuracy_score(target_train, target_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
