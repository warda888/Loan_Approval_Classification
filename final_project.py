# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vzr7G5370jIaYGz_ZGIdMHjbIusW1bVK

# Detailed Data

1. **person_age:** Age of the person
2. **person_gender:** Gender of the person
3. **person_education:** Highest education level
4. **person_income:** Annual income
5. **person_emp_exp:** Years of employment experience
6. **person_home_ownership:** Home ownership status (e.g., rent, own, mortgage)
7. **loan_amnt:** Loan amount requested
8. **loan_intent:** Purpose of the loan
9. **loan_int_rate:** Loan interest rate
10. **loan_percent_income:** Loan amount as a percentage of annual income
11. **cb_person_cred_hist_length:** Length of credit history in years
12. **credit_score:** Credit score of the person
13. **previous_loan_defaults_on_file:** Indicator of previous loan defaults
14. **loan_status (target variable):** Loan approval status: 1 = approved; 0 = rejected
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/loan_data.csv')
# pd.set_option('display.max_columns', None)

"""# Data Understanding"""

df.info()

"""The dataset contains 45,000 rows and 14 columns (features). These features include 6 with float data types, 3 with integer data types, and 5 with object data types. All data types are appropriately assigned and consistent with their intended use."""

df.head()

"""## Missing Value Check"""

missing_value = df.isnull().sum()
missing_value

"""## Duplicate Check"""

duplicate = df.duplicated().sum()
duplicate

"""There are no missing values or duplicated data in the dataset.

## Data Distribution
"""

df.describe()

"""Among the 9 features with numerical data types, there are some anomalies in the data.

1. person_age: There is a maximum age of 144 years, which is highly unrealistic. This column will be addressed in the next stage. The average age still falls within the normal range.

2. person_income: There is a significant discrepancy between the minimum and maximum annual income. The minimum annual income in the data is 8,000 dollars, and the maximum income is 7.2 million dollars, while the average annual income is around 80,000 dollars. This is not an impossible scenario.

3. person_emp_exp: There is a maximum work experience of 125 years, which is an outlier. This column will be addressed in the next stage. The average value still falls within the normal range.

4. loan_amnt: The loan amount feature still falls within a small range and no issues have been detected.

5. loan_int_rate: The loan interest rates range between 5% and 20%, with the data still falling within a normal range.

6. loan_percent_income: The percentage of income relative to the loan amount is also within a normal range.

7. cb_person_cred_hist_length: The loan history length is still within a normal category.

8. credit_score: FICO's (Fair Isaac Corporation) credit score generally range from 300 to 850, and the majority of applicants have a mid-range credit score.

9. loan_status: The average loan approval rate is very low, and this will be analyzed further in the next stage.

# Exploratory Data Analysis (EDA)

## Univariate Analysis
"""

# hitogram plot for education, home ownership, and loan intention

plt.figure(figsize=(15, 12))

categorical_columns = ['person_education', 'person_home_ownership', 'loan_intent']

for i, column in enumerate(categorical_columns, 1):
    # Calculate the frequency and sort the categories.
    ordered_categories = df[column].value_counts().index

    # ordered categorical column.
    df[column] = pd.Categorical(df[column], categories=ordered_categories, ordered=True)

    # histogram plot
    plt.subplot(3, 3, i)
    ax = sns.histplot(df[column], bins=len(ordered_categories), kde=False, color='skyblue', edgecolor='black')

    # labeled the histogram
    for p in ax.patches:
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()
        ax.text(x, y, int(y), ha='center', va='bottom', fontsize=10)

    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frekuensi')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""From the histogram plot, detailed data are as follows:

1. Person_education: There are 5 education levels: high school, associate/diploma, bachelor’s degree, master’s degree, and doctoral degree. The highest number of applications comes from individuals with a bachelor’s degree, while the fewest applications come from those with a doctoral degree. The number of applications for each level is as follows: 13,399 for bachelor’s, 12,028 for associate, 11,972 for high school, 6,980 for master’s, and 621 for doctoral.

2. Person_home_ownership: There are 4 types of homeownership: rented, mortgaged, owned, and other. From this data, the majority of applicants are renters. The data shows  23,443 rented homes, 18,489 mortgaged homes, 2,951 owned homes, and 117 homes with other ownership types.

3. Loan_intent: The purposes of loan applications vary widely, with 6 different loan intents in the data. The highest number of loan applications is for education purposes. There are 9,153 for education, 8,548 for medical purposes, 7,819 for venture/business, 7,552 applications for personal reasons, 4,783 for home improvements, and 7,145 for debt consolidation.

"""

# Piechart for gender dan loan default

# Create a figure plotting.
plt.figure(figsize=(10, 6))

# Pie chart for person_gender
plt.subplot(1, 2, 1)
gender_counts = df['person_gender'].value_counts()
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=['skyblue', 'lightcoral'], startangle=90)
plt.title('Distribution of Gender')

# Pie chart for previous_loan_defaults_on_file
plt.subplot(1, 2, 2)
loan_defaults_counts = df['previous_loan_defaults_on_file'].value_counts()
plt.pie(loan_defaults_counts, labels=loan_defaults_counts.index, autopct='%1.1f%%', colors=['lightgreen', 'orange'], startangle=90)
plt.title('Distribution of Previous Loan Defaults on File')

# show the plot
plt.tight_layout()
plt.show()

"""The piechart shows the percentage of gender and previous loan default. The detailed explanations are below:

1. Person_gender: The gender distribution is relatively balanced, with males accounting for 55.2% and females representing 44.8%.

2. Previous_loan_defaults_on_file: The status of previous loan applications shows a nearly equal proportion of rejections and approvals.
"""

# boxplot for outlier

numerical_columns = ['person_age', 'person_income', 'person_emp_exp',
                     'loan_amnt','loan_int_rate','loan_percent_income',
                     'cb_person_cred_hist_length','credit_score','loan_status']

plt.figure(figsize=(15, 12))

for i, column in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(data=df, y=column, color='skyblue')
    plt.title(f'Boxplot of {column}')
    plt.xlabel('')
    plt.ylabel(column)

plt.tight_layout()
plt.show()

"""From all the boxplots of numerical data, all features have outliers. However, outlier handling will be applied to certain features. Here’s the analysis:

1. Person_age: This feature contains outliers where loan applications were made by individuals older than 100 years, with some even reaching 140 years, which clearly represents an outlier and will be addressed in the next step. The majority of applicants, however, fall between the ages of 20 and 40 years.

2. Person_income: There is a significant variation in annual income, leading to many outliers. Although this feature contains many outliers, they will not be removed or handled.

3. Person_emp_exp: This feature contains outliers, with some individuals having more than 60 years of work experience, and some even reaching 120 years. These outliers will be addressed in the next step.

4. Loan_amnt: The loan amounts vary significantly due to different needs. Although there are many outliers, they will not be handled.

5. Loan_int_rate: In general, loan interest rates range from 5% to 20% per year depending on the type of loan and the borrower’s risk profile. Outliers in this feature will also not be addressed.

6. Loan_percent_income: Similar to annual income, outliers in loan_percent_income will not be addressed.

7. cb_person_cred_hist_length: The loan term varies significantly depending on the type of loan. Typically, loan terms range from 1 to 30 years, such as home loans which can reach up to 30 years, or education loans which typically last 1 to 10 years, and secured loans which can last up to 20 years. In this dataset, there are no home loan applications, so the outliers in this feature will be handled.

8. Credit_score: Typically, FICO credit scores range from 300 to 850. The data's interval still falls within the expected range, so outliers will not be handled. Credit scores depend on an individual’s eligibility for loans or credit from financial institutions, and are calculated based on several factors reflecting one's ability and habits in managing finances.

9. Loan_status: This feature is the target variable, so outliers will not be handled.

# Data Cleaning

## Outlier Handling
"""

# person_age and person_emp_exp

age_exp = ((df['person_age'] >= 70) | (df['person_emp_exp'] >= 50))
print(f'The number of outliers in age and work experience: {age_exp.sum()}')
print(f'The percentage of outliers in age and work experience: {age_exp.mean()*100:.2f}%')

# handling by removing the outlier

df = df[~((df['person_age'] >= 70) | (df['person_emp_exp'] >= 50))]

"""1. person_age: The age limit for applying for a loan varies significantly depending on the type of loan. Additionally, the maximum age limit is typically aligned with retirement age (55-65), but for academics, the retirement age can be extended up to 70 years. The minimum age limit is when the applicant reaches adulthood according to the law in the country (in this case, I am using 20 years old).

2. person_emp_exp: The minimum age for work experience is 0 years, while the maximum limit varies. In this case, I set the limit at 50 years, assuming an academician starts working at 20 years old and retires at 70 years old. Therefore, it can be concluded that the maximum work experience is 50 years.
"""

# cb_person_cred_hist_length

data = df['cb_person_cred_hist_length'] > 20
print(f'The number of credit length over 20 years : {data.sum()}')
print(f'The percentage of credit length over 20 years : {data.mean()*100:.2f}%')

# handling by removing the outlier

df = df[~(df['cb_person_cred_hist_length'] > 20)]

"""cb_person_cred_hist_length: Since the loans applied for are only for personal purposes, health, education, home improvements, and debt consolidation, the loan term in this case is limited to a maximum of 20 years.

# Deep Dive Analysis

## KDE plot
"""

import seaborn as sns
import matplotlib.pyplot as plt

# descriptive labels for loan_status
df['loan_status'] = df['loan_status'].map({0: 'Rejected', 1: 'Approved'})

# List of numerical variable
numeric_columns = [
    'person_age', 'person_income', 'person_emp_exp', 'loan_amnt',
    'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score'
]

# create grid plot
plt.figure(figsize=(12, 14))
n_cols = 2
n_rows = (len(numeric_columns) + 1) // n_cols

# iteration for KDE plot
for i, column in enumerate(numeric_columns, 1):
    plt.subplot(n_rows, n_cols, i)

    # KDE plot
    sns.kdeplot(
        data=df,
        x=column,
        hue='loan_status',
        fill=True,
        alpha=0.5,
        palette='Set2'
    )

    # title and labels
    plt.title(f'Distribution of {column} by Loan Status', fontsize=12)
    plt.xlabel(column.replace('_', ' ').title(), fontsize=10)
    plt.ylabel('Density', fontsize=10)

# layout
plt.tight_layout()
plt.show()

"""The KDE plot shows that the variables person_age, person_income, person_emp_exp, loan_amnt, cb_person_cred_hist_length, and credit_score have similar distribution patterns for both approved and rejected applications. However, loan_int_rate and loan_percent_income display unique patterns. Specifically, higher interest rates are associated with a greater likelihood of loan approval. Similarly, applicants with loan_percent_income (the loan amount as a percentage of annual income) above 0.3% are more likely to have their loans approved.

## Stake Barchart
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# loan_status colours
colors = ['lightseagreen', 'salmon']

# List of categorical variables
categorical_columns = [
    'person_gender', 'person_education', 'person_home_ownership',
    'loan_intent', 'previous_loan_defaults_on_file'
]

# Size and layout grid plot
plt.figure(figsize=(14, 13))
n_cols = 2
n_rows = (len(categorical_columns) + 1) // n_cols

# Plot each category
for i, column in enumerate(categorical_columns, 1):
    plt.subplot(n_rows, n_cols, i)

    # Create a pivot table
    pivot = pd.crosstab(df[column], df['loan_status'], normalize='index') * 100

    # Stacked bar chart
    bottom_values = [0] * len(pivot)
    for j, category in enumerate(pivot.columns):
        plt.bar(
            pivot.index,
            pivot[category],
            bottom=bottom_values,
            color=colors[j],
            label=category
        )
        bottom_values += pivot[category]

        # labels
        for k in range(len(pivot[category])):
            value = pivot[category].iloc[k]
            if value > 0:
                plt.text(
                    k,
                    bottom_values[k] - value / 2,
                    f"{value:.1f}%",
                    ha='center',
                    va='center',
                    fontsize=9
                )

    # Visual settings
    plt.title(f'{column.replace("_", " ").title()} by Loan Status', fontsize=12)
    plt.xlabel(column.replace('_', ' ').title(), fontsize=10)
    plt.ylabel('Percentage (%)', fontsize=10)
    plt.xticks(rotation=45)

# Add a legend
plt.legend(title='Loan Status', bbox_to_anchor=(1.05, 1), loc='upper left')

# Layout
plt.tight_layout()
plt.show()

"""For the data on person_gender and person_education, no unique patterns are evident from the bar plot.

For person_home_ownership, the majority of approved applications come from individuals who rent or have other types of housing, whereas those who own homes are more likely to be rejected. As a result, feature encoding will be performed to group individuals into those who own homes (own and mortgage) and those who do not own homes (rent and other).

Similarly, for loan_intent, the plot shows that consumptive purposes (medical, debt consolidation, and personal) have a higher approval rate compared to productive purposes (education, home improvement, and venture). Therefore, feature encoding will be done using these categories.

For the previous_loan_default_on_file data, there are no applications with a previously approved status that have been approved again.

# Pre-Processing Data

## VIF score
"""

# retransform loan_status data type to binary

df['loan_status'] = df['loan_status'].map({'Rejected': 0 , 'Approved': 1})

# split the feature and target

X = df.drop('loan_status', axis=1)
Y = df['loan_status']

# Multicollinearity check
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

numeric = df.select_dtypes(include=['number'])

x = add_constant(numeric)

vif_df = pd.DataFrame([vif(x.values, i) for i in range(x.shape[1])],
                      index=x.columns).reset_index()
vif_df.columns = ['feature', 'vif_score']

vif_df = vif_df.loc[vif_df.feature != 'const']
vif_df

"""## Heatmap Corelation"""

# heatmap correlation
import seaborn as sns
import matplotlib.pyplot as plt

numeric = pd.concat([numeric], axis=1)
corr = numeric.corr()

plt.figure(figsize=(10,7))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.show()

# drop feature person_age and cb_person_cred_hist_length
columns_to_drop = ['person_age', 'cb_person_cred_hist_length']

# delete features from data frame
df = df.drop(columns=columns_to_drop, axis=1)

"""Based on the multicollinearity check, the features person_age and person_emp_exp exhibit very high VIF values (greater than 4), indicating the need for handling multicollinearity. A heatmap also highlights a strong correlation between these two features. While person_age appears more relevant when analyzed in relation to the target (loan_status), the difference in their relationships with the target is minimal. Considering this, along with business priorities, I have decided to drop the person_age feature. cb_person_cred_hist_length will also droped."""

# Multicollinearity re-check
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from statsmodels.tools.tools import add_constant

numeric = df.select_dtypes(include=['number'])

x = add_constant(numeric)

vif_df = pd.DataFrame([vif(x.values, i) for i in range(x.shape[1])],
                      index=x.columns).reset_index()
vif_df.columns = ['feature', 'vif_score']

vif_df = vif_df.loc[vif_df.feature != 'const']
vif_df

"""## Features Encoding"""

# transform data type 'person_gender'
df['person_gender'] = df['person_gender'].map({'male':1, 'female': 0})

# transform data type 'previous_loan_defaults_on_file'
df['previous_loan_defaults_on_file'] = df['previous_loan_defaults_on_file'].map({'Yes': 1, 'No': 0})

# unique data check person_education
unique1 = df['person_education'].unique()
unique1

# transform data type 'person_education'
df['person_education'] = df['person_education'].map({
    'High School': 0,
    'Associate': 1,
    'Bachelor': 2,
    'Master': 3,
    'Doctorate': 4
    })

# unique data check 'person_home_ownership'
unique2 = df['person_home_ownership'].unique()
unique2

# transform data type 'person_home_ownership'
df['person_home_ownership'] = df['person_home_ownership'].map({
    'OTHER': 0,
    'RENT': 0,
    'OWN': 1,
    'MORTGAGE': 1
    })

# unique data check 'loan_intent'
unique3 = df['loan_intent'].unique()
unique3

# transform data type 'loan_intent'
df['loan_intent'] = df['loan_intent'].map({
    'PERSONAL': 0,
    'MEDICAL': 0,
    'DEBTCONSOLIDATION': 0,
    'EDUCATION': 1,
    'VENTURE': 1,
    'HOMEIMPROVEMENT': 1
    })

"""Feature encoding for binary categorical variables are directly transformed into numerical values of 0 and 1. The variable person_gender is encoded as male = 1 and female = 0, while previous_loan_defaults_on_file is encoded as yes = 1 and no = 0.

For the person_education variable, which consists of five categories ordered by educational level, the encoding follows a sequential numerical representation: High School = 0, Associate = 1, Bachelor = 2, Master = 3, and Doctorate = 4.

The person_home_ownership variable, which originally contains four categories, is grouped into two broader categories: individuals who own a home (own and mortgage) are assigned a value of 1, whereas those who do not (rent and other) are assigned a value of 0.

Similarly, the loan_intent variable, which comprises six categories, is categorized into two groups based on purpose: productive and consumptive. Productive loan purposes (education, venture, and home improvement) are encoded as 1, while consumptive purposes (personal, medical, and debt consolidation) are encoded as 0.

## Feature Importance
"""

from sklearn.ensemble import RandomForestClassifier

X = df.drop(columns=['loan_status'])
Y = df['loan_status']

# Building and training a Random Forest model
model = RandomForestClassifier()
model.fit(X, Y)

# feature importance
feature_importances = model.feature_importances_

# frame
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""After conducting a feature importance analysis, four key features were identified as having a significant impact and will be utilized in the machine learning model. These features are:
1. previous_loan_defaults_on_file
2. loan_int_rate
3. loan_percent_income
4. person_income

# Modelling
"""

# split the data (data train and data test)
from sklearn.model_selection import train_test_split

X = df.drop('loan_status', axis=1)
Y = df['loan_status']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

"""## K-Nearest Neighbors (KNN)

### KNN 11 features
"""

# Initialize and train the model
from sklearn.neighbors import KNeighborsClassifier

k = 3
model_knn = KNeighborsClassifier(n_neighbors=k)
model_knn.fit(X_train, Y_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

Y_train_pred = model_knn.predict(X_train)
Y_test_pred = model_knn.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### KNN 4 features"""

new_column = ['credit_score', 'loan_amnt','person_home_ownership','person_emp_exp','person_education','loan_intent','person_gender']

features_train = X_train.drop(columns = new_column, axis = 1)
target_train = Y_train

features_test = X_test.drop(columns = new_column, axis = 1)
target_test = Y_test

# Initialize and train the model
from sklearn.neighbors import KNeighborsClassifier

k = 3
model_knn = KNeighborsClassifier(n_neighbors=k)
model_knn.fit(features_train, target_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

target_train_pred = model_knn.predict(features_train)
target_test_pred = model_knn.predict(features_test)

train_accuracy = accuracy_score(target_train, Y_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""The accuracy results for the K-Nearest Neighbors (KNN) model using 11 features show a training accuracy of 90% and a test accuracy of 83%. In comparison, the KNN model utilizing only 4 features achieves the same training accuracy of 90% but a lower test accuracy of 78%.

These results indicate that the KNN model with 11 features performs better, as it achieves higher test accuracy, suggesting improved generalization to unseen data.

## Logistic Regression

### Logistic Regression 11 features
"""

# Initialize and train the model
from sklearn.linear_model import LogisticRegression

model_lr = LogisticRegression(max_iter=1000, random_state=42)
model_lr.fit(X_train, Y_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

Y_train_pred = model_lr.predict(X_train)
Y_test_pred = model_lr.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### LogisticRegression 4 features"""

# Initialize and train the model
from sklearn.linear_model import LogisticRegression

model_lr_new = LogisticRegression(max_iter=1000, random_state=42)
model_lr_new.fit(features_train, target_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

target_train_pred = model_lr_new.predict(features_train)
target_test_pred = model_lr_new.predict(features_test)

train_accuracy = accuracy_score(target_train, target_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""The accuracy results for the Logistic Regression model using 11 features show a training accuracy of 87% and a test accuracy of 88%. In contrast, the model using only 4 features achieves a slightly higher training accuracy of 88% while maintaining the same test accuracy of 88%.

These results suggest that the Logistic Regression model with 4 features is preferable, as it achieves comparable test accuracy while using fewer features, potentially reducing model complexity and improving interpretability.

## Decission Tree

### Decision Tree 11 features
"""

# Initialize and train the model
from sklearn.tree import DecisionTreeClassifier, plot_tree

model_dt = DecisionTreeClassifier(criterion='gini', max_depth=9, random_state=42)
model_dt.fit(X_train, Y_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

Y_train_pred = model_dt.predict(X_train)
Y_test_pred = model_dt.predict(X_test)

train_accuracy = accuracy_score(Y_train, Y_train_pred)
test_accuracy = accuracy_score(Y_test, Y_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(Y_test, Y_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""### Decission Tree 4 features"""

# Initialize and train the model
from sklearn.tree import DecisionTreeClassifier, plot_tree

model_dt = DecisionTreeClassifier(criterion='gini', max_depth=9, random_state=42)
model_dt.fit(features_train, target_train)

# Metrics Evaluation
from sklearn.metrics import accuracy_score

target_train_pred = model_dt.predict(features_train)
target_test_pred = model_dt.predict(features_test)

train_accuracy = accuracy_score(target_train, target_train_pred)
test_accuracy = accuracy_score(target_test, target_test_pred)

print(f"Train data accuracy: {train_accuracy}")
print(f"Test data accuracy: {test_accuracy}")

# evaluation matrics on test data
# plotting confusion matrix
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(target_test, target_test_pred)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Rejected", "Approved"], yticklabels=["Rejected", "Approved"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""The accuracy results for the Decision Tree model using 11 features indicate a training accuracy of 93% and a test accuracy of 92%. Meanwhile, the model using only 4 features achieves a slightly lower training accuracy of 91% and a test accuracy of 91%. These results suggest that the Decision Tree model with 11 features performs better, as it achieves higher overall accuracy.

Comparing the three machine learning models—K-Nearest Neighbors (KNN), Logistic Regression, and Decision Tree—the Decision Tree model demonstrates the highest test accuracy, making it the most suitable model for loan approval prediction in this case.
"""